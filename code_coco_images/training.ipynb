{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_prepare import data_processing\n",
    "if not os.path.exists('dataset'): data_processing()\n",
    "\n",
    "from data_prepare import TRAIN_IMG_PATH, VAL_IMG_PATH, TEST_IMG_PATH, TRAIN_CAPTION_PATH, VAL_CAPTION_PATH, TEST_CAPTION_PATH, VOCAB_DIR\n",
    "train_img_path = TRAIN_IMG_PATH\n",
    "val_img_path = VAL_IMG_PATH\n",
    "test_img_path =TEST_IMG_PATH\n",
    "\n",
    "train_caption_path = TRAIN_CAPTION_PATH\n",
    "val_caption_path = VAL_CAPTION_PATH\n",
    "test_caption_path = TEST_CAPTION_PATH\n",
    "vocab_dir = VOCAB_DIR\n",
    "model_path = \"models/\"  # Path where the trained model will be saved\n",
    "\n",
    "import pickle\n",
    "# Load vocabulary file\n",
    "with open(vocab_dir, 'rb') as f: vocab = pickle.load(f)\n",
    "\n",
    "# Create model directory\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.00012\n",
    "num_epochs = 5\n",
    "num_workers = 6\n",
    "\n",
    "# Increase Model Capacity\n",
    "embed_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Adjust to the correct size for Inception\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Adjust to the correct size for Inception\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "from data_loader import get_loader\n",
    "# Declare data loaders\n",
    "train_data_loader = get_loader(train_img_path, train_caption_path, vocab, train_transform, batch_size, shuffle=True, num_workers=num_workers, testing=False, pin_memory=True)\n",
    "val_data_loader = get_loader(val_img_path, val_caption_path, vocab, val_transform, batch_size, shuffle=False, num_workers=num_workers, testing=False, pin_memory=True)\n",
    "test_data_loader = get_loader(test_img_path, test_caption_path, vocab, test_transform, batch_size, shuffle=False, num_workers=num_workers, testing=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from model_vit_gpt2 import Encoder, Decoder # Import Model \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(embed_size).to(device)\n",
    "decoder = Decoder(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "# Criterion with ignore_index to skip pad tokens\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab('<pad>'))\n",
    "\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) # For ViT-GPT2\n",
    "# params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.batch_norm.parameters()) # For Resnet101-LSTM\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Gradient clipping value\n",
    "clip_value = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate(encoder, decoder, data_loader, criterion, device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in data_loader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return round(total_loss / len(data_loader), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 훈련 손실 누적\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_loss = 0  # 에포크 시작 전 훈련 손실 초기화\n",
    "    with tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as t:\n",
    "        for i, (images, captions, lengths) in t:\n",
    "            # Move batch of images and captions to GPU if available\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "            # Forward pass through encoder and decoder\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "\n",
    "            # outputs의 크기를 변환: (batch_size * sequence_length, vocab_size)\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1)) # Only for Vit-GPT2\n",
    "            \n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            # Calculate the batch loss based on the criterion\n",
    "            loss = criterion(outputs, targets)\n",
    "            train_loss += loss.item()  # 각 배치의 손실 누적\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            clip_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "            torch.nn.utils.clip_grad_norm_(clip_params, clip_value)\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tqdm's description with the current loss\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            # Log training statistics and save model checkpoints\n",
    "        if  (epoch+1) % 1 == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(model_path, 'decoder-{}.ckpt'.format(epoch+1)))\n",
    "            torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-{}.ckpt'.format(epoch+1)))\n",
    "            \n",
    "    # 에포크가 끝날 때 평균 훈련 손실 계산 및 기록\n",
    "    train_loss /= len(train_data_loader)\n",
    "    train_loss = round(train_loss, 4)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # 검증 손실 계산 및 기록\n",
    "    val_loss = evaluate(encoder, decoder, val_data_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 검증 손실 그래프 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 n-gram 수준(1-gram, 2-gram, 3-gram, 4-gram)에서 생성된 캡션과 참조 캡션 간의 일치도를 계산합니다.\n",
    "import torchtext.data.metrics as metrics\n",
    "\n",
    "encoder_path = './models/encoder-5.ckpt'\n",
    "decoder_path = './models/decoder-5.ckpt'\n",
    "\n",
    "# 모델과 데이터 로더 설정\n",
    "encoder = Encoder(embed_size).to(device)\n",
    "decoder = Decoder(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "decoder.load_state_dict(torch.load(decoder_path))\n",
    "\n",
    "# 테스트 모드 설정\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "def calculate_bleu(data_loader, encoder, decoder, vocab, device):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for images, captions, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            features = encoder(images)\n",
    "            sampled_ids = decoder.sample(features)\n",
    "            sampled_ids = sampled_ids.cpu().numpy()\n",
    "\n",
    "            # Convert word_ids to words\n",
    "            for i in range(len(images)):\n",
    "                sampled_caption = []\n",
    "                for word_id in sampled_ids[i]:\n",
    "                    word = vocab.idx2word[word_id]\n",
    "                    sampled_caption.append(word)\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                predictions.append(sampled_caption[1:-1])\n",
    "\n",
    "                # Original sentence\n",
    "                orig_caption = []\n",
    "                for word_id in captions[i].numpy():\n",
    "                    word = vocab.idx2word[word_id]\n",
    "                    orig_caption.append(word)\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                references.append([orig_caption[1:-1]])\n",
    "\n",
    "    bleu1_score = metrics.bleu_score(predictions, references, max_n=4, weights=[1, 0, 0, 0])\n",
    "    bleu2_score = metrics.bleu_score(predictions, references, max_n=4, weights=[0, 1, 0, 0])\n",
    "    bleu3_score = metrics.bleu_score(predictions, references, max_n=4, weights=[0, 0, 1, 0])\n",
    "    bleu4_score = metrics.bleu_score(predictions, references, max_n=4, weights=[0, 0, 0, 1])\n",
    "\n",
    "    # 변경: BLEU 점수 계산 결과를 다른 변수에 할당\n",
    "    return bleu1_score, bleu2_score, bleu3_score, bleu4_score\n",
    "\n",
    "# 모델의 일반화 능력 평가\n",
    "bleu_score = calculate_bleu(test_data_loader, encoder, decoder, vocab, device)\n",
    "for idx, bleu in enumerate(bleu_score):\n",
    "    print(f'BLEU {idx + 1} Score: {round(bleu, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 테스트 데이터셋에서 랜덤으로 10개의 이미지 선택\n",
    "test_images = []\n",
    "for _ in range(12):\n",
    "    idx = random.randint(0, len(test_data_loader.dataset) - 1)\n",
    "    image, _ = test_data_loader.dataset[idx]\n",
    "    test_images.append(image)\n",
    "\n",
    "# 이미지와 캡션 출력\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, image in enumerate(test_images):\n",
    "    image = image.to(device)\n",
    "    feature = encoder(image.unsqueeze(0))\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "    # 캡션 변환\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption[1:-1]).capitalize()\n",
    "\n",
    "    # 이미지 출력\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    image = np.clip(image, 0, 1)\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(sentence)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
