{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 이미지 다운\n",
    "if not os.path.exists('train2017'):\n",
    "    # 파일이 존재하지 않을 때 실행할 코드\n",
    "    !wget http://images.cocodataset.org/zips/train2017.zip\n",
    "    !unzip train2017.zip\n",
    "    \n",
    "# 캡션 다운\n",
    "if not os.path.exists('annotations'):\n",
    "    # 파일이 존재하지 않을 때 실행할 코드\n",
    "    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    !unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "# dataPrepare.py에서 데이터 비율 및 vocab을 정의할 수 있다.\n",
    "if not os.path.exists('dataset'):\n",
    "    !python3 data_prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.00012\n",
    "num_epochs = 5\n",
    "num_workers = 6\n",
    "\n",
    "# Increase Model Capacity\n",
    "embed_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from data_loader import get_loader\n",
    "from model_vit_gpt2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = [\"./dataset/train/images\", \"./dataset/val/images\", \"./dataset/test/images\"] \n",
    "\n",
    "train_img_path = img_dir[0] # resized image for training\n",
    "val_img_path = img_dir[1] # resized image for validation\n",
    "test_img_path = img_dir[2] # resized image for test\n",
    "\n",
    "coco_caption_path = \"./annotations/captions_train2017.json\"\n",
    "train_caption_path = \"./dataset/train/captions.txt\" # resized image for training\n",
    "val_caption_path = \"./dataset/val/captions.txt\" # resized image for validation\n",
    "test_caption_path = \"./dataset/test/captions.txt\" # resized image for test\n",
    "\n",
    "vocab_path = \"./dataset/vocab.pkl\"  # Path to the preprocessed vocabulary file\n",
    "# Load vocabulary file\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "model_path = \"models/\"  # Path where the trained model will be saved\n",
    "# Create model directory\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same preprocessing and normalization parameters as were applied in the pre-trained Inception model.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Adjust to the correct size for Inception\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Adjust to the correct size for Inception\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Declare data loaders\n",
    "train_data_loader = get_loader(train_img_path, train_caption_path, vocab, train_transform, batch_size, shuffle=True, num_workers=num_workers, testing=False, pin_memory=True)\n",
    "val_data_loader = get_loader(val_img_path, val_caption_path, vocab, val_transform, batch_size, shuffle=False, num_workers=num_workers, testing=False, pin_memory=True)\n",
    "test_data_loader = get_loader(test_img_path, test_caption_path, vocab, test_transform, batch_size, shuffle=False, num_workers=num_workers, testing=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = ViTEncoder(embed_size).to(device)\n",
    "decoder = GPT2Decoder(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "# Criterion with ignore_index to skip pad tokens\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab('<pad>'))\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Gradient clipping value\n",
    "clip_value = 5\n",
    "\n",
    "# Learning rate scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate(encoder, decoder, data_loader, criterion, device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in data_loader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return round(total_loss / len(data_loader), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 훈련 손실 누적\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_loss = 0  # 에포크 시작 전 훈련 손실 초기화\n",
    "    with tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as t:\n",
    "        for i, (images, captions, lengths) in t:\n",
    "            # Move batch of images and captions to GPU if available\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "            # Forward pass through encoder and decoder\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "\n",
    "            # outputs의 크기를 변환: (batch_size * sequence_length, vocab_size)\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            # Calculate the batch loss based on the criterion\n",
    "            loss = criterion(outputs, targets)\n",
    "            train_loss += loss.item()  # 각 배치의 손실 누적\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            clip_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "            torch.nn.utils.clip_grad_norm_(clip_params, clip_value)\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tqdm's description with the current loss\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            # Log training statistics and save model checkpoints\n",
    "        if  (epoch+1) % 1 == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(model_path, 'decoder-{}.ckpt'.format(epoch+1)))\n",
    "            torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-{}.ckpt'.format(epoch+1)))\n",
    "    # 에포크가 끝날 때 평균 훈련 손실 계산 및 기록\n",
    "    train_loss /= len(train_data_loader)\n",
    "    train_loss = round(train_loss, 4)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # 검증 손실 계산 및 기록\n",
    "    val_loss = evaluate(encoder, decoder, val_data_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Learning rate scheduler update는 에포크 끝에서 한 번만 수행\n",
    "    # scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 추가: 훈련 및 검증 손실 그래프 그리기\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(train_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(val_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "# 추가: 훈련 및 검증 손실 그래프 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/ImageFile.py:503\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mfileno()\n\u001b[1;32m    504\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     axes[i]\u001b[39m.\u001b[39mset_title(sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m plt\u001b[39m.\u001b[39mtight_layout()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfor04/home/whitdisa04/ArtCaptioning/code/training_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m plt\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mexplicitly there.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 446\u001b[0m \u001b[39mreturn\u001b[39;00m _get_backend_mod()\u001b[39m.\u001b[39;49mshow(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m figure_manager \u001b[39min\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         display(\n\u001b[1;32m     91\u001b[0m             figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure,\n\u001b[1;32m     92\u001b[0m             metadata\u001b[39m=\u001b[39;49m_fetch_figure_metadata(figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure)\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[39m.\u001b[39m_to_draw \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[1;32m    180\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    341\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2367\u001b[0m             filename,\n\u001b[1;32m   2368\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2369\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2370\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2371\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2372\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2233\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2234\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39m, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m FigureCanvasAgg\u001b[39m.\u001b[39mdraw(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 458\u001b[0m mpl\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimsave(\n\u001b[1;32m    459\u001b[0m     filename_or_obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_rgba(), \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mfmt, origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mupper\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m     dpi\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi, metadata\u001b[39m=\u001b[39;49mmetadata, pil_kwargs\u001b[39m=\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/image.py:1689\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m)\n\u001b[1;32m   1688\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1689\u001b[0m image\u001b[39m.\u001b[39;49msave(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:2353\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2353\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[1;32m   2354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   2355\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/PngImagePlugin.py:1397\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1396\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1397\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)])\n\u001b[1;32m   1399\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[1;32m   1400\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/ImageFile.py:507\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    505\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    506\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 507\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;49;00m, exc)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(fp, \u001b[39m\"\u001b[39m\u001b[39mflush\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    509\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/ImageFile.py:526\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m exc:\n\u001b[1;32m    524\u001b[0m     \u001b[39m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m         l, s, d \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(bufsize)\n\u001b[1;32m    527\u001b[0m         fp\u001b[39m.\u001b[39mwrite(d)\n\u001b[1;32m    528\u001b[0m         \u001b[39mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 모델과 데이터 로더 설정\n",
    "encoder = ViTEncoder(embed_size).to(device)\n",
    "decoder = GPT2Decoder(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "encoder.load_state_dict(torch.load('./models/encoder-6.ckpt'))\n",
    "decoder.load_state_dict(torch.load('./models/decoder-6.ckpt'))\n",
    "\n",
    "# 테스트 모드 설정\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 테스트 데이터셋에서 랜덤으로 10개의 이미지 선택\n",
    "test_images = []\n",
    "for _ in range(12):\n",
    "    idx = random.randint(0, len(test_data_loader.dataset) - 1)\n",
    "    image, _ = test_data_loader.dataset[idx]\n",
    "    test_images.append(image)\n",
    "\n",
    "# 이미지와 캡션 출력\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, image in enumerate(test_images):\n",
    "    image = image.to(device)\n",
    "    feature = encoder(image.unsqueeze(0))\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "    # 캡션 변환\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption[1:-1]).capitalize()\n",
    "\n",
    "    # 이미지 출력\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    image = np.clip(image, 0, 1)\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(sentence)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 1 Score: 0.3902\n",
      "BLEU 2 Score: 0.1196\n",
      "BLEU 3 Score: 0.0454\n",
      "BLEU 4 Score: 0.0196\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "각 n-gram 수준(1-gram, 2-gram, 3-gram, 4-gram)에서 생성된 캡션과 참조 캡션 간의 일치도를 계산합니다.\n",
    "이러한 각각의 n-gram 일치도에 대해 동일한 가중치(기본적으로 각각 0.25)를 부여하여, 이들의 평균을 계산합니다.\n",
    "이 평균 점수가 최종 BLEU 점수가 됩니다.\n",
    "\"\"\"\n",
    "import torchtext.data.metrics as metrics\n",
    "\n",
    "# 모델과 데이터 로더 설정\n",
    "encoder = ViTEncoder(embed_size).to(device)\n",
    "decoder = GPT2Decoder(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "encoder.load_state_dict(torch.load('./models/encoder-6.ckpt'))\n",
    "decoder.load_state_dict(torch.load('./models/decoder-6.ckpt'))\n",
    "\n",
    "# 테스트 모드 설정\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "def calculate_bleu(data_loader, encoder, decoder, vocab, device):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for images, captions, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            features = encoder(images)\n",
    "            sampled_ids = decoder.sample(features)\n",
    "            sampled_ids = sampled_ids.cpu().numpy()\n",
    "\n",
    "            # Convert word_ids to words\n",
    "            for i in range(len(images)):\n",
    "                sampled_caption = []\n",
    "                for word_id in sampled_ids[i]:\n",
    "                    word = vocab.idx2word[word_id]\n",
    "                    sampled_caption.append(word)\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                predictions.append(sampled_caption[1:-1])\n",
    "\n",
    "                # Original sentence\n",
    "                orig_caption = []\n",
    "                for word_id in captions[i].numpy():\n",
    "                    word = vocab.idx2word[word_id]\n",
    "                    orig_caption.append(word)\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                references.append([orig_caption[1:-1]])\n",
    "\n",
    "    bleu1_score = metrics.bleu_score(predictions, references, max_n=4, weights=[1, 0, 0, 0])\n",
    "    bleu2_score = metrics.bleu_score(predictions, references, max_n=4, weights=[0, 1, 0, 0])\n",
    "    bleu3_score = metrics.bleu_score(predictions, references, max_n=4, weights=[0, 0, 1, 0])\n",
    "    bleu4_score = metrics.bleu_score(predictions, references, max_n=4, weights=[0, 0, 0, 1])\n",
    "\n",
    "    # 변경: BLEU 점수 계산 결과를 다른 변수에 할당\n",
    "    return bleu1_score, bleu2_score, bleu3_score, bleu4_score\n",
    "\n",
    "# 모델의 일반화 능력 평가\n",
    "bleu_score = calculate_bleu(test_data_loader, encoder, decoder, vocab, device)\n",
    "for idx, bleu in enumerate(bleu_score):\n",
    "    print(f'BLEU {idx + 1} Score: {round(bleu, 4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
