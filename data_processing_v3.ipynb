{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-29 19:33:03--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.229.193, 52.216.50.129, 3.5.27.234, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.229.193|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘train2017.zip’\n",
      "\n",
      "train2017.zip         4%[                    ] 776.29M  16.3MB/s    eta 19m 11s"
     ]
    }
   ],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2017.zip\n",
    "!unzip train2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from PIL import Image\n",
    "import os\n",
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/whitdisa04/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "num_train_img = 2000\n",
    "num_val_img = 500\n",
    "num_test_img = 300\n",
    "word_threshold = 4\n",
    "\n",
    "image_paths = \"./train2017\" # original image\n",
    "train_img_path = \"./dataset/train/images\" # resized image for training\n",
    "val_img_path = \"./dataset/val/images\" # resized image for validation\n",
    "test_img_path = \"./dataset/test/images\" # resized image for test\n",
    "img_dir = [train_img_path, val_img_path, test_img_path] \n",
    "size = [256, 256]\n",
    "\n",
    "coco_caption_path = \"./annotations/captions_train2017.json\"\n",
    "train_caption_path = \"./dataset/train/captions.txt\" # resized image for training\n",
    "val_caption_path = \"./dataset/val/captions.txt\" # resized image for validation\n",
    "test_caption_path = \"./dataset/test/captions.txt\" # resized image for test\n",
    "vocab_path = \"./dataset/vocab.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n",
      "<ipython-input-34-2efb557ca186>:4: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize(size, Image.LANCZOS)\n"
     ]
    }
   ],
   "source": [
    "def resize_image(image_path_tuple):\n",
    "    img_path, output_dir, size = image_path_tuple\n",
    "    with Image.open(img_path) as img:\n",
    "        resized_img = img.resize(size, Image.LANCZOS)\n",
    "        resized_img.save(os.path.join(output_dir, os.path.basename(img_path)))\n",
    "\n",
    "def process_images_in_parallel(image_paths, output_dir, size, num_processes=4):\n",
    "    # 이미지 경로와 출력 디렉토리, 크기를 튜플로 묶어줍니다.\n",
    "    image_path_tuples = [(img_path, output_dir, size) for img_path in image_paths]\n",
    "\n",
    "    with Pool(num_processes) as p:\n",
    "        p.map(resize_image, image_path_tuples)\n",
    "\n",
    "if not os.path.exists(train_img_path):\n",
    "    os.makedirs(train_img_path)\n",
    "if not os.path.exists(val_img_path):\n",
    "    os.makedirs(val_img_path)\n",
    "if not os.path.exists(test_img_path):\n",
    "    os.makedirs(test_img_path)\n",
    "\n",
    "images = sorted(os.listdir(image_paths))\n",
    "num_images = len(images)\n",
    "\n",
    "# 이미지를 훈련, 검증, 테스트 세트로 분리\n",
    "train_images = images[:num_train_img]\n",
    "val_images = images[num_train_img:num_train_img + num_val_img]\n",
    "test_images = images[num_train_img + num_val_img:num_train_img + num_val_img + num_test_img]\n",
    "\n",
    "# 각 세트에 대한 이미지 경로를 생성\n",
    "train_image_paths = [os.path.join(image_paths, image) for image in train_images]\n",
    "val_image_paths = [os.path.join(image_paths, image) for image in val_images]\n",
    "test_image_paths = [os.path.join(image_paths, image) for image in test_images]\n",
    "\n",
    "# 병렬 처리를 사용하여 이미지 리사이징\n",
    "process_images_in_parallel(train_image_paths, train_img_path, size, num_processes=4)\n",
    "process_images_in_parallel(val_image_paths, val_img_path, size, num_processes=4)\n",
    "process_images_in_parallel(test_image_paths, test_img_path, size, num_processes=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_captions(json_file_path, img_dirs):\n",
    "    # JSON 파일 읽기\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 이미지 ID와 파일 경로 매핑\n",
    "    image_file_map = {item['id']: item['file_name'] for item in data['images']}\n",
    "\n",
    "    # 캡션 데이터를 이미지 ID별로 그룹화\n",
    "    caption_groups = {}\n",
    "    for item in data['annotations']:\n",
    "        image_id = item['image_id']\n",
    "        if image_id not in caption_groups:\n",
    "            caption_groups[image_id] = []\n",
    "        caption_groups[image_id].append(item['caption'].strip())\n",
    "\n",
    "    # 캡션 수 조정: 5개로 맞추기\n",
    "    for image_id in caption_groups:\n",
    "        captions = caption_groups[image_id]\n",
    "        # 5개 초과하는 경우, 처음 5개만 유지\n",
    "        if len(captions) > 5:\n",
    "            caption_groups[image_id] = captions[:5]\n",
    "\n",
    "\n",
    "    # 파일 이름에 따른 분류\n",
    "    train_captions, val_captions, test_captions = [], [], []\n",
    "    counter = Counter()\n",
    "\n",
    "    for img_dir in img_dirs:\n",
    "        file_names = set(os.listdir(img_dir))\n",
    "        for image_id, file_name in image_file_map.items():\n",
    "            if file_name in file_names:\n",
    "                captions = caption_groups.get(image_id, [])\n",
    "                for caption in captions:\n",
    "                    line = f\"{file_name},{caption}\\n\"\n",
    "                    tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "                    counter.update(tokens)\n",
    "\n",
    "                    if img_dir == img_dirs[0]:\n",
    "                        train_captions.append(line)\n",
    "                    elif img_dir == img_dirs[1]:\n",
    "                        val_captions.append(line)\n",
    "                    else:\n",
    "                        test_captions.append(line)\n",
    "\n",
    "    return train_captions, val_captions, test_captions, counter\n",
    "\n",
    "# 함수 호출\n",
    "train_captions, val_captions, test_captions, counter = process_captions(coco_caption_path, img_dir)\n",
    "\n",
    "# 결과 저장\n",
    "def save_captions(captions, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(captions)\n",
    "\n",
    "save_captions(train_captions, train_caption_path)\n",
    "save_captions(val_captions, val_caption_path)\n",
    "save_captions(test_captions, test_caption_path)\n",
    "\n",
    "# 단어 빈도에 따른 필터링\n",
    "words = [word for word, cnt in counter.items() if cnt >= word_threshold]\n",
    "\n",
    "# Vocabulary 생성 및 저장\n",
    "vocab = Vocabulary()\n",
    "vocab.add_word('<pad>')\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "vocab.add_word('<unk>')  # word frequency below threshold\n",
    "\n",
    "for word in words:\n",
    "    vocab.add_word(word)\n",
    "\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
